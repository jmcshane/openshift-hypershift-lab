HyperShift Certificate Management in OpenShift 4.18+1. Introduction to HyperShift Certificate ManagementHyperShift represents a transformative architectural shift in OpenShift Container Platform (OCP) deployments, fundamentally altering how clusters are managed and scaled. By decoupling the control plane from the data plane, HyperShift enables the hosting of OpenShift control planes as standard Kubernetes pods on a dedicated "management cluster" (also known as the hosting cluster).1 This innovative design offers substantial advantages, including reduced operational costs, significantly faster cluster provisioning times (approximately 15 minutes), enhanced portability across various cloud environments, and robust isolation between management and workload planes.5 Despite these benefits, this architectural separation introduces a unique set of considerations for certificate management, which is critical for securing all communication pathways, both within hosted clusters and with external entities.Clusters provisioned through HyperShift are fully compliant OCP clusters, ensuring compatibility with existing OCP and Kubernetes toolchains.4 This adherence to OCP standards means that many familiar certificate management concepts apply, but their implementation must account for the HyperShift-specific deployment model.Key Components Involved in Certificate HandlingEffective certificate management in a HyperShift environment necessitates a clear understanding of the roles played by various OpenShift components:Management Cluster: This is the foundational OpenShift cluster where the HyperShift Operator is deployed. It serves as the hosting environment for the control planes of all provisioned hosted clusters, managing their lifecycle and underlying resources.6Hosted Cluster: This refers to the OpenShift cluster whose control plane components (such as the API endpoint, scheduler, and controllers) run as pods within a dedicated namespace on the management cluster. Its worker nodes, which execute application workloads, constitute the data plane.2Ingress Operator: This operator is responsible for securing access to metrics for Prometheus and managing routes within both the management and hosted clusters.15 Notably, in HyperShift, the Ingress Operator for the hosted cluster resides and operates within the hosted control plane namespace on the management cluster.6API Server: As the primary interface for interacting with the Kubernetes control plane, the API server's certificate is vital for securing client connections and ensuring authenticated access.17OAuth Server: This component, running within the hosted cluster, is responsible for authentication and authorization. It is exposed through a route, requiring proper certificate configuration to ensure secure user access.6HyperShift Operator & hcp CLI: These are the essential tools provided by Red Hat for deploying, configuring, and managing hosted clusters. They are instrumental in applying configurations that directly influence certificate management, such as base domains and service publishing strategies.6The separation of control plane and data plane in HyperShift means that certificate trust and propagation mechanisms must account for two distinct environments: the internal trust within the management cluster (for hosting control plane pods) and the external and internal trust within the hosted cluster (for its API, Ingress, and worker nodes). This architectural characteristic implies that simply managing certificates on the management cluster is insufficient for the hosted cluster's operational needs. Consequently, careful planning is required for how certificates are issued, distributed, and trusted across this boundary, especially for services exposed to the hosted cluster's data plane or external clients. This also means that troubleshooting certificate issues may involve inspecting both the management cluster (where control plane pods run) and the hosted cluster (where worker nodes and applications reside).2. HyperShift Certificate ArchitectureA thorough understanding of OpenShift's default certificate architecture and its translation to HyperShift is foundational before implementing any custom certificate solutions. OpenShift clusters, including those provisioned via HyperShift, operate on a layered certificate trust model designed to secure various internal and external communication paths.Default Certificate Behavior and ComponentsThe OpenShift Ingress Operator plays a central role in securing network traffic. It automatically generates default certificates for Ingress Controllers, which serve as temporary placeholders until custom certificates are explicitly configured.15 It is critical to note that these operator-generated default certificates are explicitly not recommended for use in production environments.15The Ingress Operator leverages service serving certificates to secure access to metrics endpoints for Prometheus and to secure application routes. The Operator requests these certificates from the service-ca controller, which then places them into secrets within the openshift-ingress-operator and openshift-ingress namespaces.15 For its internal operations, the Ingress Operator generates a self-signed signing certificate, named router-ca, which is stored in the openshift-ingress-operator namespace. This router-ca is then used to sign any default ingress certificates, such as router-certs-default, which are placed in the openshift-ingress namespace. Both the default ingress certificate and its signing CA typically possess a validity period of two years.20Understanding Internal CAs and Trust ChainsOpenShift Container Platform clusters are designed with multiple internal self-signed root Certificate Authorities (CAs) that establish various chains of trust for internal components.23 For instance, the default API server certificate is issued by an internal OpenShift Container Platform cluster CA.18 While this internal trust model is robust for inter-component communication within the cluster, clients external to the cluster will not inherently trust this certificate unless the internal CA is explicitly added to their trust stores.18 The use of self-signed internal infrastructure CA certificates is considered to carry minimal risk, as their implicit trust is confined to other internal cluster components.24Distinction between Management Cluster and Hosted Cluster CertificatesIn a HyperShift deployment, certificate management operates across two distinct layers:Management Cluster Certificates: These certificates are dedicated to securing the management cluster itself. This includes its API server, its own Ingress Controller, and the various HyperShift Operator components that run on it.Hosted Cluster Certificates: These certificates secure the individual hosted clusters. This encompasses their respective API servers, Ingress Controllers responsible for application routes within each hosted cluster, and internal communication pathways among components of the hosted cluster's data plane.A key architectural detail is that the Ingress Operator responsible for the hosted cluster runs within the hosted control plane namespace on the management cluster. Conversely, the OAuth server, which handles authentication for the hosted cluster, runs inside the hosted cluster and is exposed via a route.6 This arrangement means that although the Ingress operator resides on the management cluster, the certificates it manages directly govern the hosted cluster's external accessibility.For API server exposure, HyperShift offers a "Shared Ingress" architecture. In this model, a single Load Balancer on the management cluster can serve the API servers of multiple hosted clusters. This is achieved using the PROXY Protocol, which includes a custom ClusterID Type-Length-Value (TLV) for efficient SNI-based routing and TLS passthrough.25 This centralized approach significantly influences the certificate strategy for API server endpoints, potentially consolidating them under a single multi-Subject Alternative Name (SAN) certificate on the shared Load Balancer.The documentation clearly states that default ingress certificates are intended as placeholders and are not suitable for production environments.15 This strong advisory, coupled with the absence of built-in alerts for their impending expiration 22, highlights a significant operational vulnerability. Relying on these default certificates in a production setting inevitably leads to unexpected outages and demands manual, disruptive remediation efforts, such as deleting secrets and restarting pods.20 This directly undermines HyperShift's benefits of "at scale" and "rapid provisioning." This suggests that the default certificate mechanism is primarily for quick proofs-of-concept or development environments, not for robust, long-term operations.The Shared Ingress architecture 25 centralizes API server exposure for numerous hosted clusters behind a single Load Balancer on the management cluster. This means the TLS certificate for these API endpoints will likely reside on this central Load Balancer or an associated proxy. While this centralization simplifies certificate management for the API plane by reducing the number of certificates to manage across potentially hundreds of hosted clusters, it also introduces a single point of failure. If the shared ingress's certificate or its management is mishandled, it could disrupt API access for all hosted clusters. The certificate for this shared ingress would typically need to be a multi-domain (SAN) certificate to cover all hosted cluster API endpoints, or a wildcard certificate if the domain structure permits.To provide a clear overview of where critical certificate components reside, the following table details their typical locations and management responsibilities within a HyperShift environment:Table: Key HyperShift Certificate Components and Their LocationsComponentPurposeLocation/NamespaceSecret Name (Example)Managed ByHosted Cluster Ingress Controller Default CertificateSecures application routes within hosted clusteropenshift-ingress (on hosted cluster)router-certs-defaultIngress OperatorHosted Cluster API Server CertificateSecures API access for hosted clusteropenshift-config (on hosted cluster)api-server-serving-certsAPI Server Operator/UserManagement Cluster Ingress Controller CertificateSecures management cluster routesopenshift-ingress (on management cluster)router-certs-defaultIngress OperatorInternal CA for IngressSigns default ingress certificatesopenshift-ingress-operator (on management cluster)router-caIngress OperatorInternal CA for API ServerSigns default API server certificatesopenshift-kube-apiserver-operator (on management cluster)kube-apiserver-to-kubelet-signerAPI Server OperatorService Serving CertificatesInternal service communicationVarious namespaces (e.g., openshift-ingress-operator)metrics-tls, router-metrics-certs-<name>service-ca controller3. Custom Certificate ConfigurationFor production HyperShift environments, replacing the default, self-signed certificates with custom, publicly trusted certificates is a fundamental security and operational requirement. This section outlines the necessary prerequisites and detailed procedures for configuring custom certificates for both the API server and the Ingress Controller.Prerequisites for Custom CertificatesBefore initiating the custom certificate configuration, several prerequisites must be met to ensure successful deployment and operation:A wildcard certificate, typically for the fully qualified .apps subdomain (e.g., *.apps.<clustername>.<domain>), along with its corresponding private key, must be obtained. Both files must be in PEM format.17The private key associated with the certificate must be unencrypted.17 Encrypted keys require decryption before they can be imported into OpenShift Container Platform.The certificate must include the subjectAltName extension, specifying the FQDN or wildcard domain it covers.17 This is crucial for modern browsers and clients to validate the certificate correctly.If the certificate is part of a chain (e.g., issued by an intermediate CA), the certificate file must contain all certificates in the correct order. This order mandates the server certificate (e.g., the wildcard certificate for the API server or Ingress) as the first entry, followed by any intermediate certificates, and finally concluding with the root CA certificate.17 This precise order is critical for clients to build a valid trust path.For certificates signed by a custom Certificate Authority (CA), the CA root certificate itself must be available for deployment within the cluster's trust store.26The repeated emphasis on the correct order for certificate chains—server certificate first, followed by intermediates, and then the root CA 17—highlights a common and often silent point of failure. A certificate that appears correct might fail validation if its chain is improperly concatenated. This underscores the necessity for strict adherence to this order in any automated certificate provisioning processes and for verification during troubleshooting.Configuring Custom Certificates for the API ServerThe default API server certificate, issued by an internal OpenShift CA, is not inherently trusted by external clients.18 To enable external clients to securely connect and verify the API server's identity, this default certificate must be replaced or augmented with a publicly trusted one.Procedure for API Server Custom Certificate:Create a Kubernetes Secret: Create a tls type secret in the openshift-config namespace. This secret will encapsulate the custom certificate chain and its corresponding private key.17Bashoc create secret tls <secret-name> --cert=/path/to/cert.crt --key=/path/to/cert.key -n openshift-config
Update the API Server Configuration: Patch the apiserver/cluster object to instruct the API server to use the newly created secret for specific FQDNs.17Bashoc patch apiserver cluster --type=merge -p '{"spec":{"servingCerts": {"namedCertificates":, "servingCertificate": {"name": "<secret-name>"}}]}}}'
Verify Configuration: Confirm that the apiserver/cluster object's YAML output now references the new secret.17Dynamic Reload: A notable advantage is that kube-apiserver pods dynamically reload updated certificates. This means that for certificate renewals, a new API server revision (and thus a full rollout) is not triggered; it only occurs during the initial addition of a named certificate.17Advanced API Server Exposure: HyperShift supports various servicePublishingStrategies for exposing services, including NodePort, LoadBalancer, Route, S3, or None.28 For large-scale deployments, particularly those serving hundreds of hosted clusters, a proposed ClusterIP strategy aims to circumvent the operational overhead and cost associated with provisioning hundreds of individual load balancers.28 This strategy envisions a central Load Balancer on the management cluster that leverages SNI-based routing and TLS passthrough to ClusterIP services residing in the hosted control plane namespace.28 This architectural evolution implies that the primary certificate for the API server would be managed at this centralized Load Balancer, simplifying certificate lifecycle management across numerous hosted clusters. This forward-looking optimization for very large-scale HyperShift deployments addresses the economic and management overhead of external Load Balancers, shifting complexity from per-cluster LB management to a centralized, highly optimized ingress solution.Configuring Custom Certificates for the Ingress ControllerThe Ingress Operator's default wildcard certificate for the .apps subdomain, while functional, must be replaced with a custom, trusted certificate in production environments for security and compliance.Procedure for Ingress Controller Custom Certificate:Create OpenSSL Configuration: Generate an OpenSSL configuration file (e.g., req.conf) for your wildcard certificate. Ensure the subjectAltName field correctly covers your desired wildcard domain, such as *.apps.<clustername>.<domain>.27Generate Certificate and Key: Use OpenSSL to create your wildcard certificate (e.g., server.crt) and its corresponding private key (e.g., server.key).27Concatenate Certificate Chain: Combine your application certificate, any intermediate certificates, and the root CA certificate into a single file (e.g., chain.pem). The order is crucial: application certificate first, then intermediates, then root CA.21Create Kubernetes Secret: Create a tls type secret in the openshift-ingress namespace, containing this concatenated certificate chain and the private key.21Bashoc create secret tls <secret-name> --cert=/path/to/chain.pem --key=/path/to/server.key -n openshift-ingress
Update Ingress Controller: Patch the default Ingress Controller (ingresscontrollers/default) to reference the new secret via its spec.defaultCertificate field.27Bashoc patch ingresscontroller.operator default --type=merge -p '{"spec":{"defaultCertificate": {"name": "<secret-name>"}}}' -n openshift-ingress-operator
Deploy CA Root Certificate (for signed certificates): If your certificate is signed by a custom CA, deploy the CA root certificate as a ConfigMap (e.g., my-ca) in the openshift-config namespace.21Bashoc create configmap <ca-configmap-name> --from-file=ca-bundle.crt=./carootcert.pem -n openshift-config
Update Cluster Proxy Configuration: Patch the cluster-wide proxy configuration (proxy/cluster) to include this trustedCA.21 This step is vital because it ensures that internal OpenShift components, such as auth, console, and the registry, trust the custom ingress certificate, preventing certificate errors when accessing these services internally.15This dual requirement for custom ingress certificates—updating the IngressController to serve the new certificate and updating the cluster-wide proxy/cluster trustedCA with the signing CA—is a common area where misconfigurations occur. Overlooking the trustedCA step can lead to internal service disruptions, even if external access to applications appears to function correctly.Managing and Updating Certificate ChainsThe process of updating certificates in OpenShift is designed for minimal disruption. For the API server, patching the apiserver/cluster object triggers a dynamic reload of the certificate, avoiding a full API server restart.17 For the Ingress Controller, updating the ingresscontrollers/default object causes the Ingress Controller to be redeployed using a rolling update strategy, ensuring continuous availability during the certificate update.294. Handling Nested SubdomainsManaging certificates for complex domain structures, particularly those involving nested subdomains, presents unique challenges due to the inherent limitations of standard wildcard certificates. This is especially relevant in HyperShift environments, where the default ingress behavior often results in nested subdomain configurations.Limitations of Standard Wildcard Certificates for Multi-Level SubdomainsA standard wildcard certificate, typically formatted as *.example.com, is designed to secure only single-level subdomains (e.g., blog.example.com, shop.example.com). By default, it will not cover multiple-level subdomains such as sub.sub.example.com.31Industry standards, specifically RFC 6125 (published in 2011), stipulate that only a single wildcard character (*) is permitted within a domain name, and this wildcard must occupy the left-most label. For example, *.domain.tld is considered valid, whereas sub.*.domain.tld or *.*.domain.tld are invalid.31 This restriction is primarily imposed for security reasons, as Certificate Authorities (CAs) must perform rigorous verification of SSL applications. Allowing multiple-level wildcards under a single certificate would introduce excessive variables, complicating the verification process and potentially compromising the overall security posture.31 Consequently, it is not possible to generate a Certificate Signing Request (CSR) with a format like *.*.yourdomain.com to cover more than one second-level subdomain group using a single standard wildcard certificate.31Strategies for Securing Nested SubdomainsGiven the limitations of standard wildcard certificates, specific strategies are required for securing complex nested subdomain structures:Multiple Traditional Wildcard Certificates: One viable approach involves obtaining a separate wildcard certificate for each subdomain level. For instance, one might use *.example.com for first-level subdomains, *.blog.example.com for second-level subdomains under blog, and so on. While this method offers greater flexibility and can be more cost-effective than acquiring individual certificates for every single hostname, it still entails the administrative burden of managing multiple distinct certificates.31Multi-Domain Wildcard SSL Certificates (SAN Certificates): This is widely considered the optimal solution for securing multiple domains and their subdomains across various levels with a single certificate.31 These certificates are also known as "SAN wildcards" or "UCC wildcards".32 They typically include three Subject Alternative Names (SANs) by default and can be extended to support up to 250 SANs for an additional fee.31 The advantages of using Multi-Domain Wildcard SSL Certificates are extensive coverage, cost-effectiveness, simplified management, and robust security.32 For example, a single Multi-Domain Wildcard certificate could effectively secure *.yourdomain.com, *.blog.yourdomain.com, *.news.yourdomain.com, and *.dev.yourdomain.com.31HyperShift's Default Ingress and DNS Behavior for Guest ClustersBy default, the HyperShift operator configures a KubeVirt guest cluster's ingress and DNS behavior to leverage the existing infrastructure of the underlying management cluster.34 This means that guest clusters automatically become a subdomain of the underlying OCP cluster. For example, if the management cluster's default ingress DNS entry is *.apps.mgmt-cluster.example.com, a guest cluster named guest deployed on this infrastructure will have its ingress accessible at *.apps.guest.apps.mgmt-cluster.example.com.34 This configuration inherently creates a nested subdomain structure.For this default ingress DNS to function correctly, the underlying cluster hosting the KubeVirt virtual machines must explicitly allow wildcard DNS routes. This is achieved by patching its ingresscontroller to set wildcardPolicy: WildcardsAllowed.5 It is also important to note that when utilizing the default guest cluster ingress, connectivity is restricted to HTTPS traffic over port 443, with plain HTTP traffic over port 80 being rejected.34The default HyperShift ingress behavior, which results in a nested subdomain structure like *.apps.guest.apps.mgmt-cluster.example.com 34, directly conflicts with the limitations of standard wildcard certificates that cannot cover sub.sub.example.com.31 This means that a single standard wildcard certificate issued for the management cluster's top-level *.apps domain will be insufficient to secure the application routes of hosted clusters. Operators must therefore either employ Multi-Domain Wildcard (SAN) Certificates or manage multiple traditional wildcard certificates, one for each level of subdomain requiring coverage.Customized Ingress and DNS BehaviorIn situations where the default ingress and DNS behavior is not suitable, HyperShift provides the flexibility to configure a KubeVirt guest cluster with a unique base domain at creation time. This customization, however, requires several manual configuration steps, typically involving three main phases: cluster creation, LoadBalancer creation, and wildcard DNS configuration.34 Upon successful completion of these steps, the HostedCluster will feature an ingress wildcard specifically configured for *.apps.<hostedcluster_name>.<base_domain>, providing a dedicated and customized domain for the hosted cluster's applications.345. External DNS IntegrationExternal DNS integration in HyperShift is a critical component for managing domain name resolution, particularly in environments with multiple hosted clusters. It plays a distinct role in separating DNS configurations for application workloads within hosted clusters from the DNS for control plane services exposed by the management cluster.Purpose of External DNS in HyperShiftHyperShift's architecture, which separates the control plane and data plane, leads to two independent areas requiring DNS configuration 37:Ingress for Workloads within the Hosted Cluster: This typically involves application routes under a domain like *.apps.service-consumer-domain.com.Ingress for Service Endpoints within the Management Cluster: This covers critical control plane services such as API and OAuth endpoints, often via a domain like *.service-provider-domain.com.The hostedCluster.spec.dns field dictates the DNS for workloads, while hostedCluster.spec.services.routePublishingStrategy.hostname governs the DNS for management cluster service endpoints.37 External DNS is deployed alongside the hypershift-operator in the hypershift namespace of the management cluster. It continuously monitors the cluster for Services or Routes annotated with external-dns.alpha.kubernetes.io/hostname, automatically creating corresponding DNS records.37external-dns-domain-filter ConfigurationThe external-dns-domain-filter is a pivotal configuration setting for the external-dns setup in HyperShift hosted clusters.37 This filter explicitly defines the service-level domain that external-dns is authorized to manage. During the hypershift install command, when external-dns is enabled, this filter is set using the --external-dns-domain-filter flag. For example:Bashhypershift install --external-dns-provider=aws --external-dns-credentials=route53-aws-creds --external-dns-domain-filter=service-provider-domain.com
Here, service-provider-domain.com designates the specific service-level domain within which external-dns will filter and manage DNS records.37 This external-dns-domain-filter must correspond to a pre-created external public domain, which serves as the public DNS Hosted Zone.37 When creating a HostedCluster that utilizes service hostnames, the --external-dns-domain specified in the hypershift create cluster command must precisely match the Public Hosted Zone established earlier by the external-dns-domain-filter during the hypershift install process.37 This strict matching ensures proper DNS resolution and allows for DNS indirection, directing traffic to the correct cluster endpoints.The external-dns-domain-filter plays a critical role in establishing the DNS trust boundary for HyperShift deployments. By explicitly defining the domain that external-dns will manage, it prevents unintended DNS record creation outside of the designated zone, which is essential for security and multi-cluster DNS management. This configuration ensures that DNS records for hosted cluster services are consistently managed within a controlled and pre-defined public zone, which is fundamental for seamless operation. Furthermore, the reliance on accurate DNS records means that any misconfiguration or failure in DNS management could lead to service disruptions, potentially requiring out-of-band updates to kubelet kubeconfigs on existing nodes if a load balancer for a service is lost.37Applicability with Endpoint Access TypesIt is important to understand that the ExternalDNS feature, and consequently the external-dns-domain-filter configuration, is only relevant for Public and PublicAndPrivate endpoint-access configurations.37 For a Private HyperShift setup, all endpoints are accessed via .hypershift.local domains, which contain CNAME records pointing to the appropriate Private Link Endpoint Services. In such private deployments, external-dns does not play a role in DNS management.38DNS for Control Plane ServicesKey control plane services such as the API Server, OAuthService, Konnectivity, and Ignition are exposed using a servicePublishingStrategy within the HostedCluster specification.37 By default, for LoadBalancer and Route publishing strategies, the service is published using the hostname found in the status of the LoadBalancer Service. When the Control Plane Operator (CPO) creates these Services and Routes, it annotates them with the external-dns.alpha.kubernetes.io/hostname annotation, with the value derived from the hostname field in the servicePublishingStrategy for that service type.37 This automation streamlines the creation of necessary DNS records for control plane access.6. Certificate Propagation and TroubleshootingCertificate propagation and troubleshooting are critical aspects of maintaining a healthy HyperShift environment, especially given the distributed nature of its control plane. Issues can arise from expired certificates, misconfigurations, or failures in the propagation mechanisms.Common Propagation IssuesA frequently encountered issue is the hosted cluster router serving the management cluster's certificate.20 This often stems from expired default ingress certificates or incorrect custom certificate configurations. The Ingress Operator generates default certificates as placeholders, which are not intended for production use.15 When these default certificates expire, they can cause connectivity problems, including the OpenShift web console becoming inaccessible with NET::ERR_CERT_DATE_INVALID errors.22The reliance on manual renewal processes for default certificates, which involves deleting secrets and restarting pods 20, represents a significant operational burden. This process is disruptive and prone to errors, directly counteracting the automation and scalability benefits of HyperShift. For production environments, the absence of automated alerts for expiring default certificates 22 further compounds this risk, making automated certificate management solutions essential to ensure continuous security and operational stability.Troubleshooting StepsDiagnosing certificate issues in HyperShift requires a systematic approach, often involving inspection of both the management and hosted clusters:Check Custom Certificate Configuration: Verify if a custom certificate is configured for the ingress controller by inspecting the ingresscontroller.operator default object. The absence of output from oc get ingresscontroller.operator default -n openshift-ingress-operator -o yaml | grep defaultCertificate indicates no custom certificate is configured.20Validate Certificate Validity: Check the validity period of the ingress certificate (router-certs-default) and the ingress CA (router-ca) using openssl commands to decode and inspect the certificate dates.20Examine Cluster Operator Status: Describe the authentication cluster operator (oc describe co/authentication) to identify any certificate expiry messages, such as certificate has expired or is not yet valid.20Gather Comprehensive Information: Utilize the oc adm must-gather command to collect diagnostic information from both the management cluster and the hosted cluster. This command provides cluster-scoped resources (nodes, CRDs) and namespaced resources (config maps, services, events, logs), which are invaluable for root cause analysis.36Inspect Ingress Resources: Check for any ingress resources configured using kubectl get ingress.39Consult cert-manager Troubleshooting: If cert-manager is in use, consult its troubleshooting FAQ for common issues like propagation failures.39Certificate Renewal and RotationThe Ingress Operator's self-signed default certificates and their CAs have a two-year validity and are not automatically rotated by the operator.20 If a default ingress certificate expires, the ingress CA is also likely expired, necessitating renewal of both. The workaround involves backing up existing secrets, deleting the router-ca and router-certs-default secrets from their respective namespaces (openshift-ingress-operator and openshift-ingress), and then restarting pods to trigger recreation with a new CA and certificate.20 For custom certificates, the documented replacement procedures must be followed.20For hosted control planes, administrators can trigger a restart of all control plane components for a specific HostedCluster resource, which is useful for certificate rotation. This is achieved by annotating the HostedCluster resource with hypershift.openshift.io/restart-date and a timestamp.40 This annotation provides a targeted control mechanism for certificate rotation in hosted control planes, allowing administrators to force a refresh of components that rely on updated certificates.Furthermore, the reconciliation process of a hosted cluster and its control plane can be temporarily paused by populating the pausedUntil field of the HostedCluster resource.40 This feature is particularly useful for debugging purposes or during critical operations like backing up and restoring an etcd database, allowing administrators to prevent automated changes that might interfere with manual interventions.7. Node Sizing RequirementsUnderstanding the node sizing requirements for HyperShift is crucial for efficient resource allocation and optimal performance, particularly for the management cluster that hosts the control planes.Management Cluster RequirementsTo successfully run the HyperShift Operator and host control planes, the management cluster requires a minimum of three worker nodes.3 These worker nodes can be deployed on-premise, such as on bare-metal platforms or using OpenShift Virtualization.3Hosted Control Plane Resource RequirementsFor a highly available hosted control plane, as tested with OpenShift Container Platform version 4.12.9 and later, the following resource requirements are observed 3:Pods: A highly available hosted control plane typically consists of 78 pods.3Persistent Volumes (PVs) for etcd: It requires three 8 GiB PVs specifically for the etcd database.3Minimum vCPU: The approximate minimum vCPU requirement is 5.5 cores.3Minimum Memory: The approximate minimum memory requirement is 19 GiB.3Pod Limits and DensityThe maxPods setting configured for each node significantly influences how many hosted clusters can be accommodated on a control-plane node.3 For bare-metal nodes, the default maxPods setting of 250 is often a limiting factor, allowing roughly three highly available hosted control planes per node, even if the underlying machine possesses ample compute resources.3 Increasing the maxPods value to 500 by configuring the KubeletConfig can substantially enhance hosted control plane density, enabling better utilization of available compute resources.3This observation highlights a critical trade-off between resource utilization and isolation. While increasing maxPods allows for greater density of hosted control planes on fewer physical nodes, it also means that more control planes share the same underlying host resources. This can simplify infrastructure management but may introduce potential noisy neighbor issues or resource contention if not carefully monitored. Optimizing maxPods is therefore a key strategy for maximizing the cost-efficiency of the management cluster.Resource CalculationThe maximum number of hosted control planes a cluster can host is determined by two primary methods:Request-based resource limit: This calculation is based on the CPU and memory requests specified for the hosted control plane pods. The baseline numbers (5 vCPUs and 18 GB memory for a highly available hosted control plane) are compared against the worker node resource capacities of the management cluster to estimate the maximum density.3Load-based limit: This considers the actual CPU and memory utilization of the hosted control plane pods when the Kubernetes API server of the hosted control plane is under active workload.3It is important to recognize that node sizing for HyperShift primarily concerns the management cluster's capacity to host control planes, rather than the worker nodes of the hosted clusters themselves. The hosted control planes are essentially workloads running on the management cluster's worker nodes. Therefore, resource planning should focus on ensuring the management cluster has sufficient capacity (CPU, memory, storage, and pod density) to support the desired number and scale of hosted control planes. The worker nodes of the hosted clusters, forming the data plane, have their own separate sizing considerations based on the application workloads they will run.8. Production Certificate Management Best PracticesFor production HyperShift deployments, robust and automated certificate management is paramount. Relying on default, self-signed certificates is explicitly discouraged due to their limited validity and lack of automated renewal, which can lead to service disruptions.15 Implementing a comprehensive strategy involving automated tools and careful integration with public Certificate Authorities (CAs) is essential.Automated Certificate Management with cert-managercert-manager is a powerful Kubernetes add-on that automates the management and issuance of X.509 certificates. It can obtain certificates from various Issuers, including popular public CAs like Let's Encrypt and private Issuers, ensuring certificates remain valid and are renewed automatically before expiry.42Key capabilities of cert-manager include:Automated issuance and renewal of certificates to secure Ingress with TLS.42Integration with recognized public and private CAs.42Support for secure pod-to-pod communication using mTLS with private PKI Issuers.42Comprehensive support for both web-facing and internal workload certificate use cases.42For OpenShift environments, a separate project, openshift-routes, enables cert-manager to automatically obtain certificates for OpenShift Routes, mirroring its functionality for Ingress or Gateway resources in vanilla Kubernetes.44cert-manager can be installed via the OpenShift OperatorHub using Operator Lifecycle Manager (OLM) or through Helm charts.30Configuration involves defining Issuer or ClusterIssuer resources, which represent the certificate authorities. A ClusterIssuer is cluster-scoped, meaning any secrets it references (e.g., for API credentials) are sought in the Cluster Resource Namespace, which defaults to cert-manager but can be customized.45For wildcard certificates, the DNS01 challenge type is recommended. This method involves cert-manager creating temporary TXT records in your DNS zone (e.g., AWS Route53). The CA then verifies domain ownership by querying these records, after which cert-manager cleans them up.30 This approach requires cert-manager to have API access to your DNS provider.Integrating cert-manager with AWS Route53 for DNS01 challenges typically involves configuring an AWS Role with a custom trust policy for the cert-manager service account, leveraging IAM Roles for Service Accounts (IRSA). The cert-manager ServiceAccount is then annotated with the eks.amazonaws.com/role-arn.30cert-manager is designed for automatic certificate renewal, typically attempting renewal when a certificate is two-thirds through its lifetime (for Routes).44 For enhanced security, setting rotationPolicy: Always on the certificate resource is highly recommended to ensure private keys are also rotated.30The necessity of automated certificate management tools like cert-manager for production HyperShift deployments cannot be overstated. Manual certificate renewal, particularly for the default certificates, imposes a significant operational burden and introduces a high risk of outages due to human error or oversight. Automation ensures continuous security, reduces administrative overhead, and aligns with the scalable nature of HyperShift.Integration with Public CAs (e.g., Let's Encrypt)Let's Encrypt is a widely adopted public CA that provides free, automated Domain Validation (DV) SSL/TLS certificates using the ACME protocol.48 These certificates are valid for 90 days, with renewal recommended every 60 days to ensure continuous coverage.49Certbot is a popular and recommended ACME client for obtaining and managing Let's Encrypt certificates.48For multi-cluster HyperShift environments, a strategic approach involves deploying cert-manager in a primary cluster. This primary cluster can manage certificate issuance using DNS01 validation (e.g., with Route53) and then synchronize the issued secrets to a centralized secret management solution like AWS Secrets Manager. Secondary hosted clusters can then pull these certificates using an External Secrets Operator, ensuring a single source of truth for certificate management and avoiding conflicts.51Leveraging AWS Certificate Manager (ACM)For HyperShift deployments on AWS, AWS Certificate Manager (ACM) offers a fully managed service for provisioning, managing, and deploying public and private SSL/TLS certificates.52ACM provides several benefits:Simplifies the process of obtaining certificates.52Offers no-cost certificates when integrated with other AWS services such as Amazon CloudFront, Elastic Load Balancing (ELB), and Amazon API Gateway.52Provides managed certificate renewal, automating the burdensome process of tracking and renewing certificates.52Ensures strong encryption and adherence to key management best practices for protecting and storing private keys.52ACM is suitable for securing websites, applications, and internal network communication within AWS.52 While cert-manager can integrate with Route53 for DNS01 challenges, the direct integration of ACM for certificate issuance within HyperShift is not extensively detailed in the provided materials. However, ACM's benefits for managed renewal and seamless integration with other AWS services make it a compelling option for production deployments strictly within the AWS ecosystem.14The strategic choice between using cert-manager with DNS01 validation and directly leveraging AWS ACM depends on the deployment's specific requirements. cert-manager offers greater flexibility for multi-cloud or hybrid environments, as it can integrate with various DNS providers and CAs. Conversely, ACM provides deep integration and fully managed certificate lifecycle within the AWS ecosystem, which can simplify operations for AWS-native HyperShift deployments. The decision should align with the organization's broader cloud strategy and existing tooling.Monitoring and AlertingEffective certificate management in production requires continuous monitoring and proactive alerting. Key metrics to watch include CPU and memory usage of certificate management components, error rates, and certificate processing times.43 Setting up alerts for certificates nearing expiration and for failed renewal attempts is critical to prevent outages.43 Red Hat Advanced Cluster Management (RHACM) can assist in this regard by providing information about expiring certificates through its Certificate Policy Controller.22Conclusions and RecommendationsHyperShift's innovative architecture, while offering significant advantages in scalability and cost efficiency for OpenShift deployments, introduces unique complexities in certificate management. The fundamental distinction between the management cluster and hosted cluster certificate lifecycles, coupled with the inherent nested subdomain structures in default configurations, necessitates a proactive and automated approach to certificate provisioning and renewal.Key Findings:Default Certificates are Insufficient for Production: OpenShift's operator-generated default certificates are explicitly designed as placeholders and are not suitable for production environments due to their limited validity and lack of automated renewal mechanisms.15 Relying on them leads to manual, disruptive, and error-prone renewal processes.Decentralized Trust Requires Dual Management: The separation of control plane and data plane means certificate trust must be established and managed across both the management cluster (hosting control planes) and the hosted clusters (running applications and worker nodes). This includes ensuring internal OpenShift components trust custom ingress certificates.Nested Subdomains Demand Advanced Certificate Strategies: HyperShift's default ingress behavior creates nested subdomain structures that standard wildcard certificates cannot cover. Multi-Domain Wildcard (SAN) certificates are the most practical solution for securing these complex domain hierarchies.External DNS is Critical for Multi-Cluster Access: external-dns plays a vital role in managing DNS records for both hosted cluster workloads and management cluster service endpoints, particularly for public and hybrid access patterns. Consistent DNS configuration across both layers is essential.Node Sizing Focuses on Management Cluster: Resource allocation for HyperShift primarily involves sizing the management cluster to adequately host the control planes of numerous hosted clusters, with maxPods being a key factor in optimizing density.Automation is Non-Negotiable for Production: Manual certificate management is unsustainable and risky at scale. Automated solutions like cert-manager are crucial for ensuring continuous security, compliance, and operational efficiency.Actionable Recommendations:Implement Custom Certificates from Day One: For all production HyperShift deployments, immediately replace operator-generated default certificates for both the API server and Ingress Controller with custom, publicly trusted certificates. Do not rely on the default certificates beyond initial testing or development.Standardize on Multi-Domain Wildcard (SAN) Certificates: For environments with nested subdomain structures, such as those arising from HyperShift's default ingress behavior, adopt Multi-Domain Wildcard SSL Certificates. This approach simplifies management and ensures comprehensive coverage across multiple domain levels.Automate Certificate Lifecycle Management: Integrate cert-manager into your HyperShift environment to automate the issuance, renewal, and management of all custom certificates. Leverage DNS01 challenges for wildcard certificates and consider integrating with cloud-provider-specific services like AWS Route53.Establish Robust DNS Integration: Configure external-dns with appropriate external-dns-domain-filter settings to ensure seamless and automated DNS record management for both hosted cluster applications and control plane service endpoints. Maintain strict consistency between the external-dns-domain-filter and the external-dns-domain used during cluster creation.Prioritize Trust Chain Integrity: Always ensure the correct concatenation order for certificate chains (server cert, intermediates, root CA) when creating secrets. For custom ingress certificates, remember to update both the Ingress Controller's defaultCertificate and the cluster-wide proxy/cluster trustedCA to ensure internal OpenShift components trust the new certificate.Monitor Certificate Health Proactively: Implement comprehensive monitoring and alerting for certificate expiration dates and renewal failures. Leverage tools like Red Hat Advanced Cluster Management (RHACM) to gain visibility into certificate status across your HyperShift fleet.Right-Size the Management Cluster: Carefully plan the resource allocation for your management cluster based on the anticipated number and scale of hosted control planes. Optimize maxPods settings to achieve desired density while maintaining performance and stability.By adhering to these recommendations, organizations can effectively navigate the complexities of certificate management in HyperShift, ensuring secure, reliable, and scalable OpenShift operations.